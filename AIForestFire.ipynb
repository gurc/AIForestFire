{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56253bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_df(path=\"fires_growth/fires\"):\n",
    "    with shapefile.Reader(path) as sf:\n",
    "        fields = sf.fields\n",
    "        columns = [x[0] for x in sf.fields][1:]\n",
    "        records = sf.records()\n",
    "        np_arr = np.array(records)\n",
    "        df = pd.DataFrame(columns=columns, data=np_arr)\n",
    "    \n",
    "    with shapefile.Reader(path) as sf:\n",
    "        df['burned'] = sf.shapes()\n",
    "\n",
    "    df['id'] = df['year'].astype(str) + '_' + df['fireid'].astype(str)\n",
    "    \n",
    "    df = df.sort_values(by=['id', 'dt'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d79dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    model = tf.keras.models.load_model(name + '.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9302b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fire_id(fire_ids):\n",
    "    idx = np.random.randint(len(fire_ids))\n",
    "    return(fire_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd17c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toBitArr(shape):\n",
    "    from PIL import Image, ImageDraw\n",
    "\n",
    "    bb = shape.bbox\n",
    "    # print(bb)\n",
    "    x_min = bb[0]\n",
    "    x_max = bb[2]\n",
    "    y_min = bb[1]\n",
    "    y_max = bb[3]\n",
    "\n",
    "    polygon = []\n",
    "    for point in shape.points:\n",
    "        a = (point[0] - x_min) / (x_max - x_min) * 16 + 8\n",
    "        b = (point[1] - y_min) / (y_max - y_min) * 16 + 8\n",
    "        polygon.append((a, b))\n",
    "\n",
    "    img = Image.new('L', (32, 32), 0)\n",
    "    ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
    "    mask = np.array(img)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b376cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(df, fire_id, model, visualize=False, deep=True):\n",
    "    # Отобрать записи\n",
    "    rows = df.loc[df['id'] == fire_id].sort_values(by=['dt'])\n",
    "    \n",
    "    # Промоделировать\n",
    "    state = toBitArr(rows.iloc[0]['burned'])\n",
    "    for i in range(len(rows)-1):\n",
    "        state = model(state)\n",
    "    \n",
    "    if deep:\n",
    "        state = state.numpy() > 0.5\n",
    "    \n",
    "    # Сравнить результаты моделирования с реальным процессом\n",
    "    real_state = toBitArr(rows.iloc[-1]['burned'])\n",
    "    iou = np.sum(state & real_state) / np.sum(state | real_state)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f6d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name, fire_count=100):\n",
    "\n",
    "    model = load_model(model_name)\n",
    "\n",
    "    ious = ()\n",
    "    \n",
    "    df = get_df()\n",
    "    \n",
    "    for i in range(fire_count):\n",
    "        fire_id = get_fire_id(df['id'].unique())\n",
    "        iou = simulate(df, fire_id, model)\n",
    "        ious += (iou,)\n",
    "\n",
    "    print('Среднее значение IoU:', np.mean(ious))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee98857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def draw(arr):\n",
    "    a = arr * 250\n",
    "    a += 5\n",
    "    a = a.repeat(10,axis=0).repeat(10,axis=1)\n",
    "    img = Image.fromarray(a)\n",
    "\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a922f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "ALL = \"./fires_growth/fires.shp\"\n",
    "PRIVATE = \"./fires_growth/private_fires.shp\"\n",
    "PUBLIC = \"./fires_growth/public_fires.shp\"\n",
    "\n",
    "class Raw:\n",
    "    def __init__(self, fireid, x, y):\n",
    "        self.fireid: str = fireid\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "def prepare_dataset_v1(path=\"fires_growth/fires\", dataset_name='data_v1'):\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    data = []\n",
    "    \n",
    "    df = get_df(path)\n",
    "    \n",
    "    print('Всего элементов в исходном датасете:', df.shape[0])\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        try:\n",
    "            x = toBitArr(df.iloc[i]['burned'])\n",
    "\n",
    "            if df.iloc[i]['id'] == df.iloc[i+1]['id']:\n",
    "                y = toBitArr(df.iloc[i+1]['burned'])\n",
    "            else:\n",
    "                y=x\n",
    "            \n",
    "            x_data.append(x)\n",
    "            y_data.append(y)\n",
    "            data.append(df.iloc[i]['id'])\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print('Обработано {} элементов датасета'.format(i))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    dataset = {'x': x_data, 'y': y_data, 'data': None}\n",
    "    with open(dataset_name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def prepare_dataset_v1_short(path=\"fires_growth/fires\", dataset_name='data_v1_short', count=2000):\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    data = []\n",
    "    \n",
    "    df = get_df(path)\n",
    "    \n",
    "    print('Всего элементов в исходном датасете:', df.shape[0])\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        try:\n",
    "            x = toBitArr(df.iloc[i]['burned'])\n",
    "\n",
    "            if df.iloc[i]['id'] == df.iloc[i+1]['id']:\n",
    "                y = toBitArr(df.iloc[i+1]['burned'])\n",
    "            else:\n",
    "                y=x\n",
    "            \n",
    "            x_data.append(x)\n",
    "            y_data.append(y)\n",
    "            data.append(df.iloc[i]['id'])\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print('Обработано {} элементов датасета'.format(i))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    data_result = []\n",
    "    \n",
    "    idx = np.random.randint(0, len(x_data), count)\n",
    "    for i in idx:\n",
    "        x_result.append(x_data[i])\n",
    "        y_result.append(x_data[i])\n",
    "        data_result.append(x_data[i])\n",
    "        \n",
    "    dataset = {'x': x_result, 'y': y_result, 'data': data_result}\n",
    "    with open(dataset_name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92664c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf2b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635a9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name):\n",
    "    with open(dataset_name + '.pickle', 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c58d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f87519c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_v0():\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(500):\n",
    "        x = np.random.random((1, 32, 32))\n",
    "        y = np.zeros((1, 32, 32))\n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "    \n",
    "    dataset = {'x': x_data, 'y': y_data, 'data': None}\n",
    "    with open('data_v0.pickle', 'wb') as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e571cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = tf.keras.Input(shape=[32, 32])\n",
    "    outputs = tf.keras.layers.Dense(32, activation='sigmoid')(inputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef9b23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def train_model(dataset_name, model_name, epochs=300):\n",
    "    print('Чтение датасета')\n",
    "    dataset = get_dataset(dataset_name)\n",
    "    \n",
    "    x = dataset['x']\n",
    "    y = dataset['y']\n",
    "    \n",
    "    print('Создание модели')\n",
    "    model = create_model()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    \n",
    "    print('Старт обучения модели')\n",
    "    x = np.array(x)\n",
    "    x.resize((500, 32, 32))\n",
    "    print(len(x))\n",
    "    print(x[0].shape)\n",
    "    y = np.array(y)\n",
    "    y.resize((500, 32, 32))\n",
    "    print(y.shape)\n",
    "    print(y[0].shape)\n",
    "    print('epochs', epochs)\n",
    "    model.fit(x, y, epochs)\n",
    "    \n",
    "    print('Сохранение модели')\n",
    "    model.save(model_name + '.h5')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117202da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, learning_rate, loss_fn):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss_fn(y, model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa92416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, x, y, epoch_num, loss_fn):\n",
    "    for epoch in range(epoch_num):\n",
    "        # Update the model with the single giant batch\n",
    "        train(model, x, y, learning_rate=0.1, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc8d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_display_result():\n",
    "    with shapefile.Reader(\"validate_public/validate_public\") as sf:\n",
    "        fields = sf.fields\n",
    "        columns = [x[0] for x in sf.fields][1:]\n",
    "        records = sf.records()\n",
    "        np_arr = np.array(records)\n",
    "        df1 = pd.DataFrame(columns=columns, data=np_arr)\n",
    "\n",
    "    with shapefile.Reader(\"validate_public/validate_public\") as sf:\n",
    "        df1['burned'] = sf.shapes()\n",
    "\n",
    "    with shapefile.Reader(\"result_format/result_format\") as sf:\n",
    "        fields = sf.fields\n",
    "        columns = [x[0] for x in sf.fields][1:]\n",
    "        records = sf.records()\n",
    "        np_arr = np.array(records)\n",
    "        df2 = pd.DataFrame(columns=columns, data=np_arr)\n",
    "\n",
    "    with shapefile.Reader(\"result_format/result_format\") as sf:\n",
    "        df2['burned'] = sf.shapes()\n",
    "        \n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1a1b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_v0():\n",
    "    prepare_dataset_v0()\n",
    "    train_model(dataset_name='data_v0', model_name='model_v0')\n",
    "    print('Эффективность нулевой модели: ')\n",
    "    evaluate('model_v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0a8d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_v1_full():\n",
    "    prepare_dataset_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cd6cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model_v1_private():\n",
    "#     prepare_dataset_v1(path=\"fires_growth/private_fires\", dataset_name='data_v1_private')\n",
    "#     print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#     train_model(dataset_name='data_v1_private', model_name='model_v1_private', epochs=2)\n",
    "    \n",
    "#     print('Эффективность первой модели: ')\n",
    "#     evaluate('model_v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aa5bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_v1_short():\n",
    "    prepare_dataset_v1_short(path=\"fires_growth/private_fires\", dataset_name='data_v1_short')\n",
    "    train_model(dataset_name='data_v1_short', model_name='model_v1_short', epochs=300)\n",
    "    \n",
    "    print('Эффективность первой модели(short): ')\n",
    "    evaluate('model_v1_short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3567a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего элементов в исходном датасете: 22848\n",
      "Обработано 0 элементов датасета\n",
      "Обработано 1000 элементов датасета\n",
      "Обработано 2000 элементов датасета\n",
      "Обработано 3000 элементов датасета\n",
      "Обработано 4000 элементов датасета\n",
      "Обработано 5000 элементов датасета\n",
      "Обработано 6000 элементов датасета\n",
      "Обработано 7000 элементов датасета\n",
      "Обработано 8000 элементов датасета\n",
      "Обработано 9000 элементов датасета\n",
      "Обработано 10000 элементов датасета\n",
      "Обработано 11000 элементов датасета\n",
      "Обработано 12000 элементов датасета\n",
      "Обработано 13000 элементов датасета\n",
      "Обработано 14000 элементов датасета\n",
      "Обработано 15000 элементов датасета\n",
      "Обработано 16000 элементов датасета\n",
      "Обработано 17000 элементов датасета\n",
      "Обработано 18000 элементов датасета\n",
      "Обработано 19000 элементов датасета\n",
      "Обработано 20000 элементов датасета\n",
      "Обработано 21000 элементов датасета\n",
      "Обработано 22000 элементов датасета\n",
      "Чтение датасета\n",
      "Создание модели\n",
      "Старт обучения модели\n",
      "500\n",
      "(32, 32)\n",
      "(500, 32, 32)\n",
      "(32, 32)\n",
      "epochs 300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6832\n",
      "Сохранение модели\n",
      "Эффективность первой модели(short): \n",
      "Среднее значение IoU: 0.23122263924722566\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_v1_short()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ecc2e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтение датасета\n",
      "Создание модели\n",
      "Старт обучения модели\n",
      "500\n",
      "(32, 32)\n",
      "(500, 32, 32)\n",
      "(32, 32)\n",
      "epochs 300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7356\n",
      "Сохранение модели\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1e1e22fd6d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(dataset_name='data_v1_short', model_name='model_v1_short', epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da5dfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение IoU: 0.14904981506555934\n"
     ]
    }
   ],
   "source": [
    "evaluate('model_v1_short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b26c96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline():\n",
    "    ious = ()\n",
    "\n",
    "    df = get_df()\n",
    "    model = lambda x: x\n",
    "    for i in range(100):\n",
    "        fire_id = get_fire_id(df['id'].unique())\n",
    "        iou = simulate(df, fire_id, model, deep=False)\n",
    "        ious += (iou,)\n",
    "\n",
    "    print('Среднее значение IoU:', np.mean(ious))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a121dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_neighbor(bool_arr, x, y, prev_dir):\n",
    "    '''\n",
    "    :param x: x pos\n",
    "    :param y: y pos\n",
    "    :param prev_dir: 7 - bottom, 6 - bottom right, 5 - right, 4 - up right, 3 - up, 2 - up left, 1 - left, 0 - bottom left\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    FIRE = 1\n",
    "\n",
    "    n_x = 0\n",
    "    n_y = 0\n",
    "    dir = 0\n",
    "    c_dir = abs((prev_dir + 3) % 8)\n",
    "\n",
    "    for i in range(8):\n",
    "        c_dir = abs((c_dir - 1) % 8)\n",
    "\n",
    "        if c_dir == 7 and bool_arr[y + 1][x] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x\n",
    "            n_y = y + 1\n",
    "            break\n",
    "        elif c_dir == 6 and bool_arr[y + 1][x + 1] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x + 1\n",
    "            n_y = y + 1\n",
    "            break\n",
    "        elif c_dir == 5 and bool_arr[y][x + 1] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x + 1\n",
    "            n_y = y\n",
    "            break\n",
    "        elif c_dir == 4 and bool_arr[y - 1][x + 1] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x + 1\n",
    "            n_y = y - 1\n",
    "            break\n",
    "        elif c_dir == 3 and bool_arr[y - 1][x] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x\n",
    "            n_y = y - 1\n",
    "            break\n",
    "        elif c_dir == 2 and bool_arr[y - 1][x - 1] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x - 1\n",
    "            n_y = y - 1\n",
    "            break\n",
    "        elif c_dir == 1 and bool_arr[y][x - 1] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x - 1\n",
    "            n_y = y\n",
    "            break\n",
    "        elif c_dir == 0 and bool_arr[y + 1][x - 1] == FIRE:\n",
    "            dir = c_dir\n",
    "            n_x = x - 1\n",
    "            n_y = y + 1\n",
    "            break\n",
    "    return n_x, n_y, dir\n",
    "\n",
    "\n",
    "def to_polygon(bool_arr, bbox):\n",
    "\n",
    "    x_min = bbox[0]\n",
    "    x_max = bbox[2]\n",
    "    y_min = bbox[1]\n",
    "    y_max = bbox[3]\n",
    "    x_diff = (x_max - x_min) / 16\n",
    "    y_diff = (y_max - y_min) / 16\n",
    "    x_bias = x_min - x_diff * 8\n",
    "    y_bias = y_min - y_diff * 8\n",
    "\n",
    "    start_x = 16\n",
    "    start_y = 0\n",
    "    for y in range(31, 0, -1):\n",
    "        if bool_arr[y][16] == 1:\n",
    "            start_y = y\n",
    "            break\n",
    "    # print(f\"{start_x}, {start_y}\")\n",
    "    points = [(start_x * x_diff + x_bias, start_y * y_diff + y_bias)]\n",
    "\n",
    "    x, y, dir = get_neighbor(bool_arr, start_x, start_y, 3)\n",
    "    # print(f\"{x}, {y}, {dir}\")\n",
    "    points.append((x * x_diff + x_bias, y * y_diff + y_bias))\n",
    "\n",
    "    while x != start_x or y != start_y:\n",
    "        prev_dir = dir\n",
    "        x, y, dir = get_neighbor(bool_arr, x, y, dir)\n",
    "\n",
    "        if dir != prev_dir or (x == start_x and y == start_y):\n",
    "            # print(f\"{x}, {y}, {dir}\")\n",
    "            points.append((x * x_diff + x_bias, y * y_diff + y_bias))\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd22ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d12e4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validation():\n",
    "    df1, df2 = read_and_display_result()\n",
    "\n",
    "    w = shapefile.Writer('result/result')\n",
    "    w.field('fireid', 'C')\n",
    "\n",
    "    for i in range(df1.shape[0]):\n",
    "        source_polygon = df1.iloc[i]['burned']\n",
    "        arr = toBitArr(source_polygon)\n",
    "        new_polygon = to_polygon(arr, source_polygon.bbox)\n",
    "        w.poly([new_polygon])\n",
    "        w.record(df1.iloc[i]['fireid'])\n",
    "\n",
    "    w.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3296c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_validation_result():\n",
    "    with shapefile.Reader(\"result/result\") as sf:\n",
    "            fields = sf.fields\n",
    "            columns = [x[0] for x in sf.fields][1:]\n",
    "            records = sf.records()\n",
    "            np_arr = np.array(records)\n",
    "            df3 = pd.DataFrame(columns=columns, data=np_arr)\n",
    "\n",
    "    with shapefile.Reader(\"result/result\") as sf:\n",
    "        df3['burned'] = sf.shapes()\n",
    "\n",
    "    result_polygon = df3.iloc[0]['burned']\n",
    "    arr2 = toBitArr(result_polygon)\n",
    "    draw(arr2)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44ca8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validation()\n",
    "df3 = visualize_validation_result()\n",
    "df3.head()\n",
    "draw(toBitArr(df3.iloc[1]['burned']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
